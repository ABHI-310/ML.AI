{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3892ff8d-f17e-4fe7-a913-b219c4338fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the required libraries\n",
    "# pandas helps us to work with datasets, like manipulating it, Helps us to easily explore, clean and analyze it.\n",
    "# numpy helps us to work in the form of arrays\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2cdf593-c7ca-4bd9-aab8-8bc642c1a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load training data (400 samples)\n",
    "raw_training_data = pd.read_csv('train.csv')\n",
    "\n",
    "# Load labeled subset (150 samples with known labels)\n",
    "cancer_type_labels = pd.read_csv('train_labels.csv')\n",
    "\n",
    "#Load test dataset\n",
    "unseen_test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# combining the datasets using Id as connector and using inner join which ensures only those elements are connected which have both features and labels \n",
    "# i.e only keeping the samples where cancer type is known.\n",
    "# this step is also essential since it prevents data leakage by separating the labels from features.\n",
    "training_master_df = pd.merge(raw_training_data, cancer_type_labels, on='Id', how='inner')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88f5bb1-29e3-4f28-8bd2-c5379d790366",
   "metadata": {},
   "source": [
    "'''\n",
    "X_train = training_master_df[selected_gene_cols].copy()\n",
    "X_test = unseen_test_data[selected_gene_cols].copy()\n",
    "# After merge\n",
    "training_master_df = pd.merge(raw_training_data, cancer_type_labels, on='Id', how='inner')\n",
    "\n",
    "# If both Class_x and Class_y exist, prefer Class_y\n",
    "if 'Class_x' in training_master_df.columns and 'Class_y' in training_master_df.columns:\n",
    "    training_master_df = training_master_df.drop(columns=['Class_x'])\n",
    "\n",
    "    training_master_df = training_master_df.rename(columns={'Class_y': 'Class'})\n",
    "\n",
    "training_master_df.head(5)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "990edaec-2d5c-4b20-b081-76946a0a96ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_master_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## step - 1\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# We only keep features that are gene expressions, i.e., columns starting with gene.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m gene_feature_cols \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtraining_master_df\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgene_\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Step 2 :\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# To reduce noise and missing data issues, we drop features where more than 30% of values are missing.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m missing_ratio \u001b[38;5;241m=\u001b[39m training_master_df[gene_feature_cols]\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_master_df' is not defined"
     ]
    }
   ],
   "source": [
    "## step - 1\n",
    "# We only keep features that are gene expressions, i.e., columns starting with gene.\n",
    "gene_feature_cols = [col for col in training_master_df.columns if col.startswith(\"gene_\")]\n",
    "\n",
    "\n",
    "\n",
    "# Step 2 :\n",
    "# To reduce noise and missing data issues, we drop features where more than 30% of values are missing.\n",
    "missing_ratio = training_master_df[gene_feature_cols].isnull().mean()\n",
    "selected_gene_cols = missing_ratio[missing_ratio < 0.3].index.tolist()\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Define X and y \n",
    "X_train = training_master_df[selected_gene_cols].copy()\n",
    "X_test = unseen_test_data[selected_gene_cols].copy()\n",
    "y_train = training_master_df[\"Class\"]\n",
    "test_ids = unseen_test_data[\"Id\"]\n",
    "\n",
    "# Add missing count feature (may capture signal!)\n",
    "X_train[\"missing_count\"] = X_train.isnull().sum(axis=1)\n",
    "X_test[\"missing_count\"] = X_test.isnull().sum(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Filling NAN Values \n",
    "## We use median imputation instead of mean, since gene expression data is highly skewed and may have outliers. \n",
    "## Median is more robust in such scenarios.\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "X_test = X_test.fillna(X_train.median()) \n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Scaling\n",
    "# It is used to standardised the features so that they are on same scale \n",
    "# This is done so that model does not prioritises the feature whose values are in a very large scale even though\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Step 6: Dimensionality Reduction (PCA)\n",
    "# Transforming data into principal components making them uncorellated, we remove redundant features,\n",
    "# this helps us prevent overfitting and capture the most important variance in the data \n",
    "# we keep 99% of the variance\n",
    "pca = PCA(n_components=0.99, random_state=42)  # Retain more variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"PCA reduced {X_train_scaled.shape[1]} → {X_train_pca.shape[1]} features.\")\n",
    "\n",
    "\n",
    "\n",
    "# Step 7: Model Training \n",
    "# We went with RandomForestClassifier because it gave good results without needing much tuning. \n",
    "# It handles noisy and high-dimensional data well, which is perfect for gene expression tasks \n",
    "# like this. Even though it doesn’t need scaling or PCA, we still used them — mainly to reduce \n",
    "# noise and make the data easier to work with. PCA helped turn thousands of gene features into \n",
    "# a smaller set of useful ones. This made things faster and cleaner. Overall, RandomForest \n",
    "# gave us reliable performance and was simple enough to trust.\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=25,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train_pca, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Step 8: Evaluation on Train\n",
    "# Just a sanity check — we're testing the model on the training data, not to measure real \n",
    "# performance, but just to make sure it's actually learning and not doing something totally \n",
    "# random.\n",
    "train_preds = rf.predict(X_train_pca)\n",
    "f1 = f1_score(y_train, train_preds, average='macro')\n",
    "print(f\"Training Macro F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc596877-9535-48ee-a67b-3bbf1221d973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ submission.csv is ready!\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"Id\": test_ids,\n",
    "    \"Class\": rf.predict(X_test_pca)\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"✅ submission.csv is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdac5f4-63c0-43c2-b9ec-b88fc52b5535",
   "metadata": {},
   "source": [
    "######\n",
    "### THOUGHT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9c0dd5-63ee-4dfd-a015-0bde79d8e111",
   "metadata": {},
   "source": [
    "'''\n",
    "The task was a multi-class classification problem based on high-dimensional gene expression data, where each sample (patient) had thousands of gene features.\n",
    "The goal was to accurately predict the type of cancer a patient has.\n",
    "The challenge involved working with sparse labels (only 150 labeled out of 400 training samples), handling over 20,000 gene features with many missing values, and avoiding overfitting due to the high dimensionality and small sample size.\n",
    "'''\n",
    "\n",
    "### Tried but Failed\n",
    "''' First tried Logistic Regression, but performance was too poor to be useful, even after tuning.\n",
    "\n",
    "Initially used mean to fill missing values, but later switched to median since it’s more robust to outliers, which are common in gene expression data.\n",
    "\n",
    "Started with StandardScaler but switched to RobustScaler because it handled outliers better.\n",
    "\n",
    "PCA was first set to retain 95% variance, but increasing it to 99% gave slightly better results, so the change was kept.\n",
    "\n",
    "Linear and polynomial SVMs were both tested; they were too slow and tended to overfit due to the high-dimensional input.\n",
    "\n",
    "An ensemble combining SVM and RandomForest was also attempted, but it did not outperform RandomForest alone.\n",
    "\n",
    "Deeper RandomForests with max_depth=40 were tested but showed signs of overfitting, so max_depth=25 was used instead.\n",
    "\n",
    "Even though RandomForest doesn’t require scaled or PCA-reduced input, both were applied to reduce noise and maintain compatibility across experiments.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f7628-2f9f-4a64-9df9-6b150a365c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
